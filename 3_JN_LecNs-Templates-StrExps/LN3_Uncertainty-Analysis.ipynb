{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lecture Notes document #3:  <span style=\"font-size:larger;color:blue\">**Introduction to Uncertainty Analysis**</span>\n",
    "\n",
    "This document was developed as part of a collection to support open-inquiry physical science experiments in Bachelor's level lab courses.  \n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.  Everyone is free to reuse or adapt the materials under the conditions that they give appropriate attribution, do not use them nor derivatives of them for commercial purposes, and that any distributed or re-published adaptations are given the same Creative Commons License.\n",
    "\n",
    "A list of contributors can be found in the Acknowledgements section.  Forrest Bradbury (https://orcid.org/0000-0001-8412-4091) of Amsterdam University College is responsible for this material and can be reached by email:  forrestbradbury (\"AT\") gmail.com\n",
    "******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook document introduces important concepts and techniques in <span style=\"font-size:larger;color:brown\">**Uncertainty Analysis**</span> for use in a physical science lab course.  Note that this material assumes some prior knowledge of basic descriptive statistics.\n",
    "\n",
    "><span style=\"font-size:larger;color:brown\">**Outline**</span>\n",
    ">\n",
    ">\n",
    ">- Uncertainty: what? and why?\n",
    ">\n",
    ">\n",
    ">- Uncertainty types & Probabilities \n",
    ">    - list of common uncertainties\n",
    ">    - common uncertainties in example measurement\n",
    ">    - important example:  rectangular probability density function & digital reading uncertainty\n",
    ">    - important example:  Gaussian probability density function & repeated measurement uncertainty\n",
    ">\n",
    ">\n",
    ">- Standard uncertainty $u$, Total standard uncertainty $u_{total}=\\sqrt{\\sum_i{u_i^2}}$ \n",
    ">\n",
    ">\n",
    ">- Significant figures (rounding and reporting)\n",
    ">\n",
    ">\n",
    ">- Uncertainty propagation\n",
    ">\n",
    ">\n",
    ">- Budgetting and Prioritizing\n",
    ">\n",
    ">\n",
    ">- Averaging & Integration time\n",
    ">\n",
    ">\n",
    ">- TEASER:  Curve fitting & Fourier transform\n",
    ">\n",
    ">\n",
    ">- Acknowledgements\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*******\n",
    "# Uncertainty:  what? and why?\n",
    "\n",
    "In the natural sciences, most quantities are represented as continuous, real numbers.  As examples, one might ask:  What is the temperature?  What is the growth rate?  What is the equilibrium density?  And one might devise experiments to measure such quantities.  *Note that the quantity to be measured is called a* ***measurand***.\n",
    "\n",
    "But, despite our best efforts, perfect knowledge of the measurand will be unattainable.  As Figure 1 below shows, one can continually add more and more resolution to a measuring apparatus (be it digital or analog), and yet always have - at best - an uncertainty in the readings at a level that is just below the scale's resolution.  If in theory a perfect measurement were somehow attainable, we see that perfect knowledge would require infinite time and resources to obtain a measurement with infinitely-fine resolution!\n",
    "\n",
    "<img src=\"includedimagesfolder/readingsUCT.png\" width=\"600\">\n",
    "\n",
    ">Figure 1.  Reading uncertainties for digital and analog scales decrease as resolution increases, but are never eliminated.  And, these reading uncertainties are only one of the many kinds of measurement uncertainty.\n",
    "\n",
    "\"The poor will always be with us\" is a saying that might originate in the Christian Bible and has been used and misused for many purposes.  With some similarities to poverty, \"uncertainty\" in experimental science is something that is usually impossible to completely eliminate, and yet is an experimenter's task to understand and minimize.  This Jupyter Notebook introduces the relevant concepts, the mathematical treatment, and good practices for reporting and minimizing experimental uncertainties.\n",
    "\n",
    "<span style=\"font-size:larger;\">**Uncertainty versus Error**</span>\n",
    "\n",
    "You may see the words \"uncertainty\" and \"error\" used interchangeably in the context of experimental measurements.  However, the word \"error\" presupposes that there is a known correct answer.  It's important to realize that, according to the scientific method, any \"correct\" answer can only be known to the extent that is can be demonstrated by measurement.  One may indeed calibrate an experimental measurement method by comparing its outcomes to a more precise method (thereby giving an approximation of the less precise method's \"error\").  But the more precise method will still have some (lower level of) uncertainty, thus we must seek to understand uncertainty in its many forms.\n",
    "\n",
    "*NOTE:  Some Maker Lab exercises involve measurements of \"g\", the apparent acceleration due to Earth's gravity.  UNLESS we are willing to question the published and accepted values of \"g\", and therefore forget the common idea that it has a known true value, then these exercises are NOT really scientifically-motivated experiments (but rather investigations of the accuracy & precision of our measurement methods and tools).*\n",
    "\n",
    "<span style=\"font-size:larger;\">**Uncertainty in reading versus Uncertainty of measurement**</span>\n",
    "\n",
    "We will see that Figure 1 above only scratches the surface.  While one can claim that the last row's digital reading is *exactly* 83.362 grams, we must realize that this does *NOT* mean that the physical quantity is exactly this value.  Rather, it is *approximately* that value with some level of uncertainty which depends on the resolution of the scale, the tolerance/calibration/rating of the measurement apparatus, and other uncertainties in the experimental setup.  We will learn how to account for these and more in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****\n",
    "# Uncertainty types and Probabilities\n",
    "\n",
    "There can be many reasons to be uncertain of a scientific measurement, all of which should be - to the extent possible -  quantitatively understood and minimized by the researcher.  \n",
    "\n",
    "In the following subsections, we list some common uncertainties, show how some apply in an example measurement, and then discuss in greater detail two important types of uncertainty in relation to their probabilities.\n",
    "\n",
    "*************\n",
    "\n",
    "## List of common uncertainties\n",
    "\n",
    "**Some typical sources of uncertainty:**\n",
    "- Type B:\n",
    "    - resolution of readings\n",
    "        - digital display:  smallest displayed digit\n",
    "        - analog readout (needle/scalebar/etc):  estimation from analog scale markers\n",
    "        - reading of analog-to-digial-converter (e.g. Arduino's 10-bit ADC \"analog\" readings):  least significant bit\n",
    "    - manufacturer's tolerance or rating of measurement apparatus\n",
    "    - self-check or re-calibration of measurement apparatus\n",
    "       - if carefully conducted, can replace the original manufacturer rating\n",
    "       - is there a zero offset?, equipment degradation?, or even improved performance within controlled conditions?\n",
    "    - assumptions or approximations in experimental setup\n",
    "        - parameters assumed to be constant\n",
    "        - certain effects assumed to be neglibile\n",
    "        - back-action (act of measurement somehow affects measurand)\n",
    "    - inherent variations in counting stochastic occurrences of quantized objects or events (Poisson distribution)\n",
    "- Type A:\n",
    "    - measurement variations in repeated experiments\n",
    "        - sometimes these are directly attributable to Type B uncertainties (like reading uncertainties or inherent variations)\n",
    "        - but often these arise due to uncontrolled environmental conditions that fluctuate across measurements \n",
    "- **Note** that the curve-fitting and Fourier-transform Jupyter Notebook documents give further examples of uncertainties which are determined via these two data analysis methods \n",
    "**************\n",
    "\n",
    "Occasionally some of the above sources may be difficult, a priori, to predict or estimate; which is why the last one is often important!  Many repeated measurements allow a researcher to quantify uncertainties due to sources that are not fully understood nor accounted for.\n",
    "\n",
    "*************\n",
    "\n",
    "## Common uncertainties in example measurement\n",
    "\n",
    "To give concrete examples of the above list, let's consider **how sources of uncertainty could affect a measurement of the period of a pendulum when using an IR break-beam setup** (as depicted in Figure 2):\n",
    "\n",
    "<img src=\"includedimagesfolder/pendulum.png\" width=\"200\">\n",
    "\n",
    ">Figure 2.  Image depicting pendulum whose period of oscillation is monitored by an IR break-beam system, where measurement uncertainties of this period of oscillation are listed in the table below.\n",
    "\n",
    "| list of uncertainties | types of uncertainty | why they affect measurement of the pendulum's period\n",
    "|--- |:---|:------: \n",
    "| | readings   | time resolution of the *timed_readings* Arduino Sketch is 4 milliseconds (thus: sampling rate = 250 Hz)\n",
    "| | tolerance   | similar un-calibrated resonator clocks have uncertainties of 1% or less (according to manufacturer's tolerances)\n",
    "| | (re)calibration   | supppose a researcher finds their Arduino's micros() timer function is off by ~0.1% or less over 10 minutes run-time (limited by uncertainty in the calibration experiment), then this self-performed calibration replaces the manufacturer's tolerance\n",
    "| | parameter assumptions   | e.g. IR break-beam emitter/detector assumed stationary\n",
    "| | ignored effects   | [precession](https://en.wikipedia.org/wiki/Precession \"wikipedia link\") of partially elliptical or non-perpendicular swings?\n",
    "| | perturbation via measurement   | the measurement back-action in this case is negligible because photons absorbed from IR beam negligibly affect momentum\n",
    "\n",
    "Identifying sources of uncertainty is only the beginning of an uncertainty analysis.  It's critical to quantify them!  And, fully and properly quantifying them means identifying their associated ***probability density functions*** (aka **PDF**'s).  In the following two subsections, we consider two commonly encountered probability density functions for describing uncertainty:  the rectangular and Gaussian curves.\n",
    "\n",
    "## Rectangular density function (and digital reading uncertainty)\n",
    "\n",
    "Any time we have maximum and minimum values that a measurement can differ from a true value, but have no reason to expect that values in this range are more or less probable, so the PDF is a **rectangular density function**.  It is easy to understand why this is appropriate for a digital reading which has been rounded off to the smallest given digit, as shown in Figure 3:\n",
    "\n",
    "<img src=\"includedimagesfolder/rectangular_pdf.png\" width=\"700\">\n",
    "\n",
    ">Figure 3.  Rectangular probability density function, shown here specifically for the uncertainty in true values of a mass that corresponds to a digital reading of 83.36 grams.\n",
    "\n",
    "Besides the displays like shown above, the readings from any ADC (analog-to-digital converter) will have similar uncertainties associated with their digitized readings.  In fact, an Arduino Uno's analog input pins use 10-bit ADCs to digitize voltage readings (see:  https://www.arduino.cc/en/Reference.AnalogRead).  Figure 4 below shows how actual voltage values (\"analog input\" on the $x$-axis) would be mapped onto digitized readings for a simpler 3-bit ADC (which only has $2^3=8$ different digital values available).\n",
    "\n",
    "<img src=\"includedimagesfolder/ADC_readinguncertainty.png\" width=\"400\">\n",
    "\n",
    ">Figure 4.  The Least Significant Bit (LSB) or the reading uncertainty is best understood by a mapping (digitization) of a continuous set of input signal values between some reference zero point and the range's full scale (FS) onto a limited set of integers expressed by a given number of bits.  This image is courtesy of Analog Devices, and shows the mapping for a 3-bit ADC which only has $2^{3} = 8$ possible readings.  Arduino's analog pins, on the other hand, have much more precise 10-bit ADC's which map readings to $2^{10} = 1024$ integers.  Figure credit:  https://www.analog.com/en/education/education-library/data-conversion-handbook.html#\n",
    "\n",
    "Similar to the decimal fraction steps in a digital display, **ADC readings have rectangular density functions for their uncertainties** with full widths equal to the Least Significant Bit (LSB) steps between possible readout values.  Our Arduino Uno's have 10-bit ADC's with a default range of 0-5V, meaning the LSB steps between readings are $\\frac{5-0}{1023} = 4.9$ mV.  Thus, a reading of 742 bits would correspond to $\\frac{742}{1023}\\cdot 5$ V $ = 3.6266$ V with a rectangular density function extending from $3.6217$ V and to $3.6315$ V (or, $ = 3.6266\\pm (0.0049)$ V).\n",
    "\n",
    "\n",
    "## Gaussian density function (and repeated measurement uncertainty)\n",
    "\n",
    "This subsection describes how a Gaussian probability density function is used in reporting uncertainty in the average of repeated measurements.\n",
    "\n",
    "In contrast to the reading uncertainties in the subsection above, sometimes the maximum and minimum differences are difficult to estimate or constrain, but it's still possible to find *typical* or *expected* differences.  This is true, for instance, if we estimate the effect of unknown and un-controllable variations in our experimental conditions with **repeated measurements**.  In repeated measurements, we may never find the absolute maximum or minimum possible measurement values, but we will eventually get a very good idea of the typical spread in measurement values.  \n",
    "\n",
    "The Central Limit Theorem tells us that when sufficiently large numbers of measurements are taken, their average estimates *the actual average that would be obtained with an infinite set of such measurements* with an uncertainty which is given by a **Gaussian density function**.  This is important, because this \"actual mean\" of an infinite set of measurements will eliminate all uncontrollable sources of uncertainty which are symmetric (i.e. equal probability of negative and positive influence).  This Gaussian PDF is centered at the measurements' average value $= \\overline{x}$ and has a half-width $u_{repM}$ equal to the standard deviation of the mean of the measurements: $$u_{repM} = \\sqrt{\\frac{\\sum\\limits_i{\\left(x_i - \\overline{x}\\right)^2}}{N\\left(N-1\\right)}}$$  In the next section, we will explain our choice of the name $u_{repM}$.\n",
    "\n",
    "\n",
    "Figure 5 shows a Gaussian PDF given by:  $$p\\left(\\overline{x_{actual}}\\right)=\\frac{1}{u_{repM} \\sqrt{2\\pi}} e^{ -\\frac{1}{2}\\left(\\frac{\\overline{x_{actual}}-\\overline{x}}{u_{repM}}\\right)^2}$$ where $\\overline{x_{actual}}$ is the average that would be obtained with an infinite set of such measurements.\n",
    "\n",
    "<img src=\"includedimagesfolder/Gaussian_pdf.png\" width=\"700\">\n",
    "\n",
    ">Figure 5.  Gaussian probability density function for possible values of the actual mean where the center is the sample average $\\overline{x}$ and the half-width is the standard deviation of the mean of the measurements $u_{repM}$.  The \"half-width\" is half the width of the peak (and equal to the square root of the Gaussian's second moment) and the name we have given it $\\left(u_{repM}\\right)$ will be explained in the next section.\n",
    "\n",
    "*NOTE:  The unbiased sample variance (taken from a set or \"sample\" of measurements) divides the sum of squared deviations through by $N-1$ instead of $N$ because dividing by the latter under-estimates the population's actual variance.  The standard deviation of measurements:   $$STDEV = \\sqrt{VAR} = \\sqrt{\\frac{\\sum\\limits_i{\\left(x_i - \\overline{x}\\right)^2}}{\\left(N-1\\right)}}$$ must subsequently by divided by $\\sqrt{N}$ to get $u_{repM}$, the experimental standard deviation of the mean (sometimes called the standard error of the mean).*  \n",
    "\n",
    "The animation in Figure 6 gives an example of how 1200 measurements can be progressively plotted along the vertical axis (upper left) and binned into a bar graph with a red line denoting the average(upper right), and how the measurements' standard deviation (lower left) and the standard deviation of the mean of the measurements (lower right) evolve as more and more measurements are included.  You should notice that $u_{repM}$, the standard deviation of the mean (lower right), decreases with a $1/\\sqrt{N}$ dependence as more measurements are included in the sample average.\n",
    "\n",
    "\n",
    "<video controls src=\"includedimagesfolder/noise_big.mp4\" width=\"600\">animation</video>\n",
    "    \n",
    ">Figure 6.  Animation showing the effect of including more and more measurements in a sample (see text for descriptions).  Freek Pols contributed to the code for making this video.\n",
    "\n",
    "****\n",
    "\n",
    "*NOTE that this and the previous subsections only considered uncertainties in readings and due to repeated measurements, but in measurements we will ALSO need to consider the other kinds of uncertainties listed at the beginning of this section!  Looking into how several kinds of uncertainties combine to give a total uncertainty is the topic of the next section.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*******\n",
    "# Standard uncertainty $\\ u\\  $, and Total standard uncertainty    $\\ u_{total}=\\sqrt{\\sum\\limits_i{u_i^2}}$ \n",
    "\n",
    "As we saw above, there can be several significant sources of uncertainty for a given measurement or set of measurements.  While we may be tempted to add up the maximum possible uncertainties from all types and report this as our total uncertainty, that is actually an overly pessimistic view on the results.  Better is to quantify a typical or \"standard uncertainty\".\n",
    "\n",
    "The **standard uncertainty** (denoted by $u$) is a characteristic width of the corresponding probability density function (PDF) which will typically capture the true value.  Mathematically, it is defined to be the square root of the PDF's second moment about its mean value:\n",
    "\n",
    "$$ \n",
    "u = \\sqrt{second.moment} =  \\sqrt{\\ \\int\\limits_{x_{min}}^{x_{max}}{\\left(x-x_{mean}\\right)^2 \\ PDF(x)\\  dx}} \n",
    "$$\n",
    "\n",
    "\n",
    "## Standard uncertainty for Gaussian density function\n",
    "\n",
    "For the **Gaussian distribution, the standard uncertainty** is equal to the bell curve's half-width and Figure 5 above shows that the actual value will be found within this distance from the center 68% of the time.  This could be shown by finding the square root of the Gaussian PDF's second moment and integrating the PDF in this region around the center (though we skip these calculations here).\n",
    "\n",
    "\n",
    "## Standard uncertainty for rectangular density function\n",
    "\n",
    "The \"typical\" or **standard uncertainty for a rectangular density function** is also found in the same way.  The calculations are simpler for the rectangular PDF because of its constant probability between the two limits.  When centered at $x_{cntr}$ with a full width of $2b$, the rectangular probability density is given by:\n",
    "$$\n",
    "rect\\_PDF (x) = \\left\\{ \\begin{array} {ll}\n",
    "\\frac{1}{2b} & \\left|x-x_{cntr}\\right|\\leq b \\\\ 0 & \\left|x-x_{cntr}\\right|>b \\\\ \\end{array} \\right.\n",
    "$$\n",
    "Using this to find the standard uncertainty for a rectangular PDF yields:\n",
    "$$ \n",
    "u_{_{rectangular}} = \\sqrt{second.moment} =  \\sqrt{\\ \\int\\limits_{x_{cntr}-b}^{x_{cntr}+b}{\\left(x-x_{cntr}\\right)^2 \\frac{1}{2b} dx}} = \\frac{b}{\\sqrt{3}}\n",
    "$$\n",
    "\n",
    "*It is interesting to note that for a rectangular distribution, we expect that $58\\% \\ \\left( = \\left(2b/\\sqrt{3}\\right)/(2b) = 1/\\sqrt{3} \\right)$ of the time the true value of the measurand will be within this standard uncertainty of the distribution's mid-point (instead of $68\\%$ for Gaussian!)  The square root of the second moment is used for both cases, but we see clearly that it yields a different coverage probability for different density functions.*\n",
    "\n",
    "\n",
    "## Examples:  standard uncertainties due to readings & repeated measurements\n",
    "\n",
    "For Type A uncertainties involving many measurements (and the Gaussian distribution), we see now that the standard uncertainty is simply equal to the **experimental standard deviation of the mean** (also known as the **standard error**):\n",
    "\n",
    "$$\n",
    "u_{_{Type\\ A}}= u_{repM}= \\sqrt{\\frac{\\sum\\limits_i{\\left(x_i - \\overline{x}\\right)^2}}{N\\left(N-1\\right)}}\n",
    "$$\n",
    "\n",
    "*Remember that the \"experimental standard deviation of the mean\" decreases as more and more measurements are averaged together which means the uncertainty from repeated measurements decays with a $1/\\sqrt{N}$ dependence (as demonstrated in Figure 6 above!)*\n",
    "\n",
    "For digital reading uncertainties, on the other hand, the rectangular distribution of total width $\\left( 2b \\right)$ yields a standard uncertainty of:\n",
    "\n",
    "$$\n",
    "u_{_{digital-reading}}= \\frac{b}{\\sqrt{3}}\n",
    "$$\n",
    "\n",
    "***\n",
    "\n",
    "The following code cell demonstrates some ways of calculating standard uncertainties for rectangular and Gaussian distributions.  (Use which-ever you like, or find your own method :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "since a +-0.05 reading uncertainty has a rectangular distribution, \n",
      "its standard uncertainty is u_rect = 0.02886751345948129\n",
      "\n",
      "\n",
      "now consider this data set:  [2.8, 1.2, 3.9, 5.2, 4.3, 2, 3.8, 3.3, 4.2, 5.4, 3.5, 2.9]\n",
      "the mean of the measurements is:  3.5416666666666665\n",
      "and the standard uncertainty quantified via repeated measurements is:\n",
      "standard uncertainty if 'data' Gaussian distributed, u_{repM} = 0.3512873151392999  using method #1\n",
      "standard uncertainty if 'data' Gaussian distributed, u_{repM} = 0.3512873151392999  using method #2\n",
      "standard uncertainty if 'data' Gaussian distributed, u_{repM} = 0.3512873151392999  using method #3\n",
      "standard uncertainty if 'data' Gaussian distributed, u_{repM} = 0.3512873151392999  using method #4\n"
     ]
    }
   ],
   "source": [
    "## Python code for finding standard uncertainties \n",
    "## for reading uncertainty and repeated measurement uncertainty\n",
    "\n",
    "# standard uncertainty for rectangular distribution is easy!\n",
    "# let's say our digital reading gives 3.5 (and no hundredths digit)\n",
    "b = 0.05 # half-width of rectangular distribution, from mid to max/min\n",
    "u_rect = b / 3**.5 #  that's all!  just b divided by square root of 3\n",
    "print(\"since a +-0.05 reading uncertainty has a rectangular distribution, \")\n",
    "rect_string=\"its standard uncertainty is u_rect = \"\n",
    "print(rect_string + str(u_rect))\n",
    "\n",
    "\n",
    "print(\"\\n\")  #prints empty line to separate results of calculations\n",
    "\n",
    "\n",
    "# the \"repeated measurement uncertainty\" = \"standard deviation of the mean\"\n",
    "# there are several equivalent methods for calculating it:\n",
    "\n",
    "# say we start with the data set given by the list:\n",
    "data = [2.8, 1.2, 3.9, 5.2, 4.3, 2, 3.8, 3.3, 4.2, 5.4, 3.5, 2.9]\n",
    "print(\"now consider this data set:  \"+ str(data))\n",
    "\n",
    "#1.this direct method uses lists (covered in LECTURENOTES_2_PythonC) \n",
    "#  and comprehensions (in LECTURENOTES_2_PythonD)\n",
    "mean = sum(data)/len(data)\n",
    "stdevmean1 = (sum( [ (x-mean)**2 for x in data ] ) / (len(data)*(len(data)-1)) )**.5  \n",
    "\n",
    "#2. method using \"statistics\" library\n",
    "import statistics                          # note: also has statistics.mean() function\n",
    "unbiased_stdev2 = statistics.stdev(data)  \n",
    "stdevmean2 = unbiased_stdev2 / (len(data))**.5  \n",
    "\n",
    "#3. method using \"numpy\" library\n",
    "import numpy as np                         # note: also has np.mean() function\n",
    "numpydata = np.array(data) # converts list to ndarray for use with Numpy functions\n",
    "unbiased_stdev3 = np.std(numpydata, ddof=1)  # ddof=1 sets divisor=N-1 for unbiased stdev\n",
    "stdevmean3 = unbiased_stdev3 / (len(data))**.5  \n",
    "\n",
    "#4. method using \"scipy.stats\" library\n",
    "from scipy import stats                    # note: also has stats.mean() function\n",
    "stdevmean4 = stats.sem(data)  # this function directly gives stdev of the mean\n",
    "\n",
    "\n",
    "# now we print results from all four methods (and find they yield the same answers!)\n",
    "print(\"the mean of the measurements is:  \"+ str(mean))\n",
    "print(\"and the standard uncertainty quantified via repeated measurements is:\")\n",
    "gaus_string=\"standard uncertainty if 'data' Gaussian distributed, u_{repM} = \"\n",
    "\n",
    "print(gaus_string + str(stdevmean1) + \"  using method #1\")\n",
    "print(gaus_string + str(stdevmean2) + \"  using method #2\")\n",
    "print(gaus_string + str(stdevmean3) + \"  using method #3\")\n",
    "print(gaus_string + str(stdevmean4) + \"  using method #4\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "## Standard uncertainties from other sources \n",
    "\n",
    "The examples above use two of the most common probability density functions (PDFs), but there are many other sources of measurement uncertainty.\n",
    "\n",
    "For some manufacturer-provided accuracy estimates or calibrations, the measurement tolerances may already be reported in terms of standard uncertainties (thus sparing you the work of calculating them from PDFs).  However, for some other types of uncertainties, it may be difficult to assign exact probability density functions.  In these cases, for the sake of completing a Maker Lab project, you are welcome to approximate the corresponding standard uncertainties by using either a Gaussian or a rectangular PDF.  You should then explain your reasons for doing so (e.g. because you either know a worst-case scenario similar to the min/max of the rectangular function or you have found some typical differences that could map onto a Gaussian bell-curve)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****\n",
    "## Total (standard) uncertainty\n",
    "\n",
    "When there are multiple sources of uncertainty which have each been quantified with a standard uncertainty ($u_i$), then the measurement's total standard uncertainty is the square root of their sum of squares:\n",
    "\n",
    "$$\n",
    "u_{total}=\\sqrt{\\sum\\limits_i{u_i^2}} = \\sqrt{u_{_{Type\\ A}}^2+u_{_{digital-reading}}^2+u_{_{calibration}}^2+u_{_{etc}}^2+...}\n",
    "$$\n",
    "\n",
    "It's important to note that this result is LESS than what one finds when the standard uncertainties are simply added together.  The reasoning for this is that it's relatively unlikely that all of the effects would conspire to all push the difference in the same direction.  Remember:  the **standard uncertainty** describes ***typical*** differences of our measurement compared to the true value, and such rare worst case scenarios would distort and over-estimate the typical differences.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******\n",
    "# Significant figures (rounding and reporting)\n",
    "\n",
    "After finding the total standard uncertainty, we can report the results of our measurement!  It doesn't make sense to report many significant figures beyond those which you have some certainty of.  For the Maker Lab projects, we ask that you keep two significant figures in your total standard uncertainty, and write the measurement's estimation down to the same level of precision (i.e. round it to the same digit).\n",
    "\n",
    "Thus, the following measurements and total standard uncertainties should be rounded as follows:\n",
    "\n",
    "| measurement | total standard uncertainty | rounded with appropriate significant figures\n",
    "|--- |---|----------\n",
    "| $V_{est}$ = 13908 Volts | $u_{total}$ = 523.89 Volts  | $V_{est}$ = 13910$\\pm$520 Volts \n",
    "| $m_{est}$ = 0.00199438 kg | $u_{total}$ = 0.001148 kg  | $m_{est}$ = 0.0020$\\pm$0.0011 kg \n",
    "| $T_{est}$ = 134.7847 $^o$C | $u_{total}$ = 20.98 $^o$C  | $T_{est}$ = 135$\\pm$21 $^o$C \n",
    "\n",
    "Alternatively, using scientific notation, the same table should be represented as follows, with the total standard uncertainties put between parentheses:\n",
    "\n",
    "| measurement | total standard uncertainty | rounded with appropriate significant figures\n",
    "|--- |---|----------\n",
    "| $V_{est}$ = 13908 Volts | $u_{total}$ = 523.89 Volts  | $V_{est}$ = 1.391(52)x10$^4$ Volts \n",
    "| $m_{est}$ = 0.00199438 kg | $u_{total}$ = 0.001148 kg  | $m_{est}$ = 2.0(1.1)x10$^{-3}$ kg \n",
    "| $T_{est}$ = 134.7847 $^o$C | $u_{total}$ = 20.98 $^o$C  | $T_{est}$ = 1.35(21)x10$^{2}$ $^o$C \n",
    "\n",
    "Either of the two methods in the tables above is a perfectly acceptable way to report uncertainty and significant figures of your results.  \n",
    "\n",
    "*NOTE:  Besides using standard uncertainties, there are also other ways:  involving confidence intervals with certain coverage factors (in a statistics course, you likely learned to calculate 95% confidence intervals).  You are welcome to also report such an **expanded uncertainty** if you think it's useful for your results, but this extra detail is not necessary.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******\n",
    "# Uncertainty propagation\n",
    "\n",
    "Often in experimental science, we try to infer a value through measurements of other ones.  In this case, the value of interest is assumed to be given by some function, $f(A, B, C, ....)$, of the other values we have measured.  However, each of these other values will have total standard uncertainties ($u_A , u_B, u_C, ...$) associated with their experimental determination...  So, how do we then find the uncertainty with which we know the outcome of $f(A, B, C, ....)$ ?  In the limit of relatively small $u$'s, the following first order formula yields a good approximation:\n",
    "\n",
    "$$\n",
    "u_f \\approx \\sqrt{ \\left(\\frac{\\partial f}{\\partial A}\\ u_A \\right)^2 + \\left(\\frac{\\partial f}{\\partial B}\\ u_B \\right)^2 + \\left(\\frac{\\partial f}{\\partial C}\\ u_C \\right)^2 + ... }\n",
    "$$\n",
    "\n",
    "This formula involves *partial derivatives* of the function $f$ with respect to each of its variables.  A partial derivative is just like a regular derivative with the simplification that all other variables are assumed to be unchanging constants for the sake of the differentiation, and it tells the slope of the function $f$ along a change of its parameter-space limited to changing one variable (for example, $\\frac{\\partial f}{\\partial A}$ gives $f$'s rate of change along the $A$ axis direction).  \n",
    "\n",
    "For the case of a function of a single variable, this approximation using the derivative is easier to visualize.  Figure 7 below demonstrates for a certain function how the function's slope at the estimation point yields an estimate of the uncertainty in function output when that slope is multiplied by the uncertainty in the input variable: \n",
    "\n",
    "<img src=\"includedimagesfolder/function_uncertainty.png\" width=\"300\">\n",
    "\n",
    ">  Figure 7.  Uncertainty propagation using the first order (linear) approximation in the case of a function with a single variable.  Credit for original figure:  Krolla, Bernd & Gava, Christiano & Pagani, Alain & Stricker, Didier. (2014). Consistent Pose Uncertainty Estimation for Spherical Cameras. \n",
    "\n",
    "The little Gaussian distributions on the horizontal and vertical axes in Figure 7 represent the expected distributions of differences with respect to a true value, and thus show how an uncertainty in $x$ leads to an uncertainty in the output of function $f(x)$.  The figure further shows how a first order approximation (a straight line through the estimation point with a slope equal to the derivative there) yields a convenient way to estimate uncertainty in $f(x)$.  For clarity, we stress in the following mathematical formula that the function's derivative(s) is(are) evaluated at the point of estimation (here: $x_m$, where $m$ stands for measured):\n",
    "\n",
    "$$\n",
    "u_f \\ \\ \\approx \\ \\ \\sqrt{ \\left(\\frac{\\partial f}{\\partial x}\\ u_x \\right)^2 + ... } \\ \\ = \\ \\ \\ \\sqrt{ \\left(\\frac{\\partial f(x_m)}{\\partial x}\\ u_x \\right)^2 + ... }\n",
    "$$\n",
    "\n",
    "## Pendulum and \"g\" example\n",
    "\n",
    "Let's consider an attempt to estimate \"g\" (the apparent acceleration due to Earth's gravity) by measuring the period of a simple pendulum.  The relation for a pendulum's period at small swing angles: $T\\approx 2\\pi \\sqrt{\\frac{L}{g}}$ can be turned around to give a function for \"g\":\n",
    "\n",
    "$$\n",
    "g \\approx \\frac{4 \\pi^2 L}{T^2}\n",
    "$$\n",
    "\n",
    "In the language used above, we have $g$ as a function of variables $L$ and $T$:  $g = g(L,T)$, and can therefore calculate its standard uncertainty as follows:\n",
    "\n",
    "$$\n",
    "u_g \\ \\ \\approx \\ \\  \\sqrt{ \\left(\\frac{\\partial g}{\\partial L}\\ u_L \\right)^2 + \\left(\\frac{\\partial g}{\\partial T}\\ u_T \\right)^2 } \\ \\ = \\ \\ \\ \\sqrt{ \\left(\\frac{4 \\pi^2 }{T_m^2} \\ u_L \\right)^2 + \\left(\\frac{-8 \\pi^2 L_m}{T_m^3} \\ u_T \\right)^2 }\n",
    "$$\n",
    "\n",
    "where $L_m$ and $T_m$ are the measured values of length and period, respectively.\n",
    "\n",
    "## Python shortcut\n",
    "\n",
    "There is actually a Python library called `uncertainties` which takes most of the calculation work out of uncertainty propagation:  https://pythonhosted.org/uncertainties/   and   https://pythonhosted.org/uncertainties/user_guide.html\n",
    "\n",
    "Below are some examples of how it can be (installed &) used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting uncertainties\n",
      "  Downloading https://files.pythonhosted.org/packages/2a/c2/babbe5b16141859dd799ed31c03987100a7b6d0ca7c0ed4429c96ce60fdf/uncertainties-3.1.2.tar.gz (232kB)\n",
      "Building wheels for collected packages: uncertainties\n",
      "  Building wheel for uncertainties (setup.py): started\n",
      "  Building wheel for uncertainties (setup.py): finished with status 'done'\n",
      "  Created wheel for uncertainties: filename=uncertainties-3.1.2-cp37-none-any.whl size=96462 sha256=66339f861c45285d4bdc2a3d07a0764a377abd68827b76656595744597faf792\n",
      "  Stored in directory: C:\\Users\\Forrest\\AppData\\Local\\pip\\Cache\\wheels\\d9\\d3\\0e\\5b0b743a8abd50373705427438456da5dc2621891138d7a618\n",
      "Successfully built uncertainties\n",
      "Installing collected packages: uncertainties\n",
      "Successfully installed uncertainties-3.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "## these installation commands need to be un-commented and ran (just once)\n",
    "## before using the uncertainties package:\n",
    "\n",
    "\n",
    "## EITHER:\n",
    "\n",
    "# #pip install --upgrade uncertainties\n",
    "\n",
    "## OR:\n",
    "\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install --upgrade uncertainties\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.00+/-0.20\n",
      "0.91+/-0.08\n",
      "2 significant figures on the uncertainty: 0.909+/-0.083\n"
     ]
    }
   ],
   "source": [
    "## UNCERTAINTY PROPAGATION EXAMPLE\n",
    "## ufloat() defines a float variable to have a value AND standard uncertainty\n",
    "## later calculations with that variable give propagated uncertainties\n",
    "\n",
    "from uncertainties import ufloat\n",
    "from uncertainties.umath import *  # sin(), etc.\n",
    "x = ufloat(1.00, 0.10)  # x = 1.00 +- 0.10\n",
    "print(2*x)\n",
    "print(sin(2*x))  \n",
    "\n",
    "## unfortunately, uncertainties doesn't automatically give two sig figs for uncertainty\n",
    "## but, we can force it to do so using a string including '{:.2u}' & .format()\n",
    "print('2 significant figures on the uncertainty: {:.2u}'.format(sin(2*x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g : 9.16+/-0.21 m/s/s\n"
     ]
    }
   ],
   "source": [
    "## ANOTHER UNCERTAINTY PROPAGATION EXAMPLE\n",
    "## now let's use this to calculate uncertainty in \"g\"\n",
    "## (assuming L and T are measured as given below)\n",
    "\n",
    "from uncertainties import ufloat\n",
    "import math\n",
    "L = ufloat(0.5530, 0.005)  # L in meters (was 55.3 +- 0.5 cm)\n",
    "T = ufloat(1.544, 0.016)  # T in seconds (was 1.544 +- 0.0156 s)\n",
    "\n",
    "print('g : {:.2u}'.format(4*math.pi**2 *L / T**2) + ' m/s/s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extended uncertainty propagation example\n",
    "\n",
    "The result above is a bit alarming, as this estimate of $g_{est}=9.16\\pm (0.21)$ m/s/s is much farther from g's accepted value than the calculated standard uncertainty would typically allow for.  Remember that if we properly identified and accurately quantified all the uncertainties, the interval $[(9.16-0.21) , (9.16+0.21)]$ should capture the accepted value a majority of the time.  So, either we are very unlucky, or there are some large un-identified sources of uncertainty that make either $u_L$ or $u_T$ much larger, or maybe our mathematical relation (which was for small swing angles) is not precise enough.  \n",
    "\n",
    "We can quickly test the last theory (\"mathematical relation not precise enough\").  We insert the first couple correction terms for finite swing angles into the formula and get:\n",
    "\n",
    "$$\n",
    "g \\approx \\frac{4 \\pi^2 L}{T^2}\\left( 1+\\frac{1}{16}\\theta_{max}^2 + \\frac{11}{3072}\\theta_{max}^4 \\right)^2\n",
    "$$\n",
    "\n",
    "We could calculate the effects by hand using:\n",
    "\n",
    "$$\n",
    "u_g \\ \\ \\approx \\ \\  \\sqrt{ \\left(\\frac{\\partial g}{\\partial L}\\ u_L \\right)^2 + \\left(\\frac{\\partial g}{\\partial T}\\ u_T \\right)^2 + \\left(\\frac{\\partial g}{\\partial \\theta_{max}}\\ u_{\\theta_{max}} \\right)^2 }\n",
    "$$\n",
    "\n",
    "But, the `uncertainties` package is quicker :) \n",
    "\n",
    "Here, let's assume the maximum swing angle is calculated through the inverse sine of ratio of horizontal swing distance over length, and the horizontal swing distance was measured to be $35\\pm 1$ cm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max swing angle in degrees: 39.3+/-1.4 deg\n",
      "g : 9.43+/-0.21 m/s/s\n"
     ]
    }
   ],
   "source": [
    "##EXTENDED UNCERTAINTY PROPAGATION EXAMPLE\n",
    "from uncertainties import unumpy  # this sub-library has arcsin\n",
    "\n",
    "L = ufloat(0.5530, 0.005)  # L in meters (was 55.3 +- 0.5 cm)\n",
    "T = ufloat(1.544, 0.016)  # T in seconds (was 1.544 +- 0.0156 s)\n",
    "swing_distance = ufloat(0.350, 0.01) # in meters:  35+-1 centimeters\n",
    "swing_radians = unumpy.arcsin(swing_distance / L) # theta in radians \n",
    "print('max swing angle in degrees: {:.2u}'.format(swing_radians*180/math.pi) + ' deg')\n",
    "\n",
    "g = (1+swing_radians**2/16)*4*math.pi**2 *L / T**2  #just using the first correction term\n",
    "print('g : {:.2u}'.format(g) + ' m/s/s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g : 9.43+/-0.21 m/s/s\n"
     ]
    }
   ],
   "source": [
    "##REPEAT WITH EXTRA CORRECTION TERM FOR FINITE SWING ANGLES:\n",
    "\n",
    "g = (1+swing_radians**2/16+11*swing_radians**4/3072)*4*math.pi**2 *L / T**2  # extra term\n",
    "print('g : {:.2u}'.format(g) + ' m/s/s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that using the extra correction term in the bottom code's calculation does not change the estimate of \"g\" when compared to its total measurement uncertainty, so we therefore don't need to worry about including any further higher order (and thus smaller) corrections.\n",
    "\n",
    "These results now look a bit more reasonable.  The 68% confidence interval (assuming the total uncertainty follows a Gaussian distribution) $[(9.43-0.21),(9.43+0.21)]$ doesn't include $9.8$, but the 95% confidence level  $[(9.43-2\\cdot 0.21),(9.43+2\\cdot 0.21)]=[9.01,9.85]$ m/s/s does!  \n",
    "\n",
    "This more exact expression which better approximates \"g\" in the case of non-negligible swing angles has significantly changed the final result for \"g\".  Thus, we see that for the large swing angles, we need to use a more exact form of the relation such that our theoretical model's (im)precision doesn't limit our experimental conclusions.\n",
    "\n",
    "***\n",
    "\n",
    "***Note for completeness:*** this document does not deal with calculating Type A uncertainties that arise in estimating parameters from curve-fitting of measured data.  This will instead be covered in a subsequent Jupyter Notebook on curve-fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******\n",
    "# Budgetting and Prioritizing\n",
    "\n",
    "In the uncertainty propagation section above, we see that a value which is calculated from estimates of several measurands has an uncertainty which depends on the uncertainties of all the measurands.  If we want to improve our experimental methods in order to obtain a more precise estimate, then where should we start?\n",
    "\n",
    "\n",
    "## Uncertainty budget\n",
    "\n",
    "An **uncertainty budget** is a great tool for improving our experiments, as it helps to prioritize our efforts.  It is essentially just a table listing where the uncertainties come from and their magnitudes.  For the function $f(A,B,C,...)$ that depends on measurands $A$, $B$, $C$, etc, the uncertainty budget table is:\n",
    "\n",
    "| variable | uncertainty contribution, in $f$'s units\n",
    "|--- |----------\n",
    "| $A$  | $\\left|\\frac{\\partial f}{\\partial A}\\ u_A \\right|$ \n",
    "| $B$  | $\\left|\\frac{\\partial f}{\\partial B}\\ u_B \\right|$ \n",
    "| $C$  | $\\left|\\frac{\\partial f}{\\partial C}\\ u_C \\right|$ \n",
    "| ...  | ... \n",
    "| $f$'s total propagated uncertainty  | $u_f \\approx \\sqrt{ \\left(\\frac{\\partial f}{\\partial A}\\ u_A \\right)^2 + \\left(\\frac{\\partial f}{\\partial B}\\ u_B \\right)^2 + \\left(\\frac{\\partial f}{\\partial C}\\ u_C \\right)^2 + ... }$ \n",
    "\n",
    "The last line reminds us that these uncertainties are not simply added, but rather squared and summed before a square root is taken!  Due to this, all but the very largest one or two of the uncertainty contributions can usually be left-alone in our attempts to improve an experiment.  If a contribution is $5x$ smaller than the largest, then its square is $25x$ smaller (!) than the square of the largest; hence we should focus only on the largest contributions in our attempt to reduce $u_f$.\n",
    "\n",
    "Besides Uncertainty Budgets for propagated uncertainties, we should also compare different types of uncertainties for individual variables, and thereby learn which measurement uncertainties deserve the most attention.  For a given measurand, $X$, let's assume we have quantified the following uncertainties:\n",
    "\n",
    "| uncertainty type | estimate of standard uncertainty\n",
    "|--- |----------\n",
    "| calibration  | $u_{X-cal} = 8.5$ \n",
    "| type A (repeated measurements)  | $u_{X-typA} = 3.1$ \n",
    "| reading  | $u_{X-read} = 0.29$ \n",
    "| $X$'s total uncertainty  | \\begin{align} u_{X-total} &= \\\\ &=\\sqrt{u_{X-cal}^2+u_{X-typA}^2+u_{X-read}^2} \\\\ &= 8.9\\end{align} \n",
    "\n",
    "In this example, the standard uncertainty from the reading $\\left(\\frac{0.5}{\\sqrt{3}}\\right)$ is more than an order of magnitude smaller than the others, thus when squaring and summing its contribution to the total is two orders of magnitude smaller.  It can be safely ignored!\n",
    "\n",
    "Instead, we should look into whether the calibration can be performed with lower uncertainty, and if so, then also work to reduce the type A uncertainty due to variation across repeated measurements (by taking and averaging together more measurements).  Remembering that the \"experimental standard deviation of the mean\" is just the sample standard deviation divided by $\\sqrt{N}$, we see that taking 100x more measurements is expected to lower this type A standard uncertainty by 10x - and at that point there will be limited returns because the reading uncertainty will eventually start to dominate.\n",
    "\n",
    "## Example with pendulum and \"g\"\n",
    "\n",
    "Let's again consider attempts to estimate \"g\" by measuring the period of simple pendulum.  For completeness, we will use the more exact relation involving the maximum angle of the swings:\n",
    "\n",
    "$$\n",
    "g \\approx \\frac{4 \\pi^2 L}{T^2}\\left( 1+\\frac{1}{16}\\theta_{max}^2 + \\frac{11}{3072}\\theta_{max}^4 \\right)^2\n",
    "$$\n",
    "\n",
    "In the Python script above, we used the `uncertainties` package to find:  $u_g \\ = \\ 9.43(21)$ m/s/s.  Here, we will set up and fill in the Uncertainty Budget table to see which variables (and their uncertainties) deserve our prioritized attention:\n",
    "\n",
    "| variable | propagated uncertainty contribution to $u_g$\n",
    "|--- |----------\n",
    "| $L$  | $\\left|\\frac{\\partial f}{\\partial L}\\ u_L \\right|$ \n",
    "| $T$  | $\\left|\\frac{\\partial f}{\\partial T}\\ u_T \\right|$ \n",
    "| $\\theta_{max}$  | $\\left|\\frac{\\partial f}{\\partial \\theta_{max}}\\ u_{\\theta_{max}} \\right|$ \n",
    "| $g$'s total propagated uncertainty  | $u_g \\approx \\sqrt{ \\left(\\frac{\\partial g}{\\partial L}\\ u_L \\right)^2 + \\left(\\frac{\\partial g}{\\partial T}\\ u_T \\right)^2 + \\left(\\frac{\\partial g}{\\partial \\theta_{max}}\\ u_{\\theta_{max}} \\right)^2 }$ \n",
    "\n",
    "We can mis-use the `uncertainties` package as shown below to pretend in these three separate calculations that there is only uncertainty coming from a single variable while holding the others fixed (and thus calculate each of these contributions):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "just from L : \t 9.434+/-0.079 m/s/s\n",
      "just from T : \t 9.43+/-0.20 m/s/s\n",
      "just from max angle : \t 9.434+/-0.019 m/s/s\n"
     ]
    }
   ],
   "source": [
    "## mis-using uncertainties package to make Uncertainty Budget\n",
    "\n",
    "# first for contribution from L's uncertainty:\n",
    "L = ufloat(0.5530, 0.005)  # L in meters (was 55.3 +- 0.5 cm)\n",
    "T = ufloat(1.544, 0.000)  # T in seconds (was 1.544 +- 0 s)\n",
    "swing_distance = ufloat(0.350, 0.0) # in meters:  35+-0 centimeters\n",
    "swing_radians = unumpy.arcsin(swing_distance / L) # theta in radians\n",
    "g = (1+swing_radians**2/16+11*swing_radians**4/3072)*4*math.pi**2 *L / T**2\n",
    "print('just from L : \\t {:.2u}'.format(g) + ' m/s/s')\n",
    "\n",
    "# second for contribution from T's uncertainty:\n",
    "L = ufloat(0.5530, 0.000)  # L in meters (was 55.3 +- 0 cm)\n",
    "T = ufloat(1.544, 0.016)  # T in seconds (was 1.544 +- 0.0156 s)\n",
    "swing_distance = ufloat(0.350, 0.0) # in meters:  35+-0 centimeters\n",
    "swing_radians = unumpy.arcsin(swing_distance / L) # theta in radians\n",
    "g = (1+swing_radians**2/16+11*swing_radians**4/3072)*4*math.pi**2 *L / T**2\n",
    "print('just from T : \\t {:.2u}'.format(g) + ' m/s/s')\n",
    "\n",
    "# finally for contribution from max swing's uncertainty:\n",
    "L = ufloat(0.5530, 0.000)  # L in meters (was 55.3 +- 0 cm)\n",
    "T = ufloat(1.544, 0.000)  # T in seconds (was 1.544 +- 0 s)\n",
    "swing_distance = ufloat(0.350, 0.01) # in meters:  35+-1 centimeters\n",
    "swing_radians = unumpy.arcsin(swing_distance / L) # theta in radians \n",
    "g = (1+swing_radians**2/16+11*swing_radians**4/3072)*4*math.pi**2 *L / T**2\n",
    "print('just from max angle : \\t {:.2u}'.format(g) + ' m/s/s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copying the uncertainty contributions from the code output into our table, we have:\n",
    "\n",
    "| variable | propagated uncertainty contribution to $u_g$, in m/s/s\n",
    "|--- |----------\n",
    "| $T$  |  0.20\n",
    "| $L$  |  0.079\n",
    "| $\\theta_{max}$  | 0.019  \n",
    "| $g$'s total propagated uncertainty   | 0.21  \n",
    "\n",
    "It's now very clear that, from the uncertainties we have identified, our calculation of \"g\" will be most improved by reducing the uncertainty in our estimation of the period, $T$.\n",
    "\n",
    "*NOTE:  While this prioritization amongst the identified uncertainties is always very useful, beware that we will only actually improve our experimental result if we have found and properly accounted for all of the sizable uncertainties!*\n",
    "\n",
    "Assuming our list does indeed contains and accurately quantify all important sources of uncertainty, we must now focus on improving our measurement of the period ($T$):  In directly measuring the period, there were a number of uncertainties which we should now compare in an Uncertainty Budget:\n",
    "\n",
    "| types of uncertainty | description | contribution to $u_T$, in s\n",
    "|--- | --- |----------\n",
    "| parameter assumptions   | e.g. IR break-beam emitter/detector assumed stationary | ?\n",
    "| ignored effects   | [precession](https://en.wikipedia.org/wiki/Precession \"wikipedia link\") of partially elliptical or non-perpendicular swings? | ?\n",
    "| tolerance   | un-calibrated resonator clocks have typical uncertainties of 1% or less | 0.01544\n",
    "| readings   | time resolution of the Arduino Sketch | 0.00231 \n",
    "| perturbation via measurement   | photons absorbed from IR beam affect momentum | negligible\n",
    "| $T$'s total uncertainty   | ***only accounting for the two quantified sources:*** | $\\begin{align} u_T &= \\sqrt{0.0154^2+0.00231^2}\\\\ &=0.156 \\end{align}$\n",
    "\n",
    "This table suggests we should try to estimate/quantify the unknown effects and if they turn out to be negligible, then perform our own calibration of our Arduino's timer to see if we can improve on the 1% uncertainty from the manufacturer's tolerance, because currently it is by far the biggest uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*******\n",
    "# Averaging & Integration time\n",
    "\n",
    "Several types of uncertainties can be reduced by simply taking more data!\n",
    "\n",
    "## Averaging \n",
    "\n",
    "For type A uncertainty (involving the dispersion of measurement values when the same experiment is conducted many times) the standard uncertainty has been defined to be the experimental standard deviation of the mean:\n",
    "\n",
    "$$\n",
    "u_{_{Type\\ A}}= \\sqrt{\\frac{\\sum\\limits_i{\\left(x_i - \\overline{x}\\right)^2}}{N\\left(N-1\\right)}} = \\frac{1}{\\sqrt{N}}\\sqrt{\\frac{\\sum\\limits_i{\\left(x_i - \\overline{x}\\right)^2}}{\\left(N-1\\right)}}\n",
    "$$\n",
    "\n",
    "The second term in the relation on the right is the standard deviation of the dataset, and we don't expect it to change so much as more and more measurements are taken.  But, the first term obviously makes $u_{_{Type\\ A}}$ smaller with a $\\frac{1}{\\sqrt{N}}$ dependence.  This reduction in uncertainty $u_{_{Type\\ A}}$ makes sense because our average value ought to become a better estimate if we average together more and more measurements.\n",
    "\n",
    "## Integration over longer times\n",
    "\n",
    "Much the same way that more measurements will give more precise results, we can also (for some experiments) simply integrate over a longer time for achieving greater precision.  Consider an estimation of a pendulum's period of oscillation.  Instead of measuring the time it takes to complete one period, we could instead measure the time for 10 or even 100 periods, and then divide the total time and also the uncertainty to get a better estimate for a single period.  \n",
    "\n",
    "This is especially useful when the timing resolution is constant (like when the measurement uncertainty is not dependent on the amount of time, like for a reading uncertainty), but DOES NOT work if the timing resolution is mostly limited by a fixed percentage (like when a clock consistently runs too slow or too fast).\n",
    "\n",
    "**Note** the two methods above (averaging and time integration) both involve spending more time taking data.  When both are possible to perform, understanding whether one or the other (or neither) is preferrable depends on the main source of measurement uncertainty!\n",
    "\n",
    "| main source of uncertainty | best method | reasoning\n",
    "|--- | --- |----------\n",
    "| reading uncertainty   | integrate over longer times | if taken $C$ times longer, then integration gives $\\frac{u_{read}}{C}$ while averaging $C$ measurements gives $\\frac{u_{read}}{\\sqrt{C}}$ (where we assume $u_{typA}$ is simply equal to $u_{read}$) \n",
    "| other type A   | integrate over longer times | both integration and averaging give $\\frac{u_{typA}}{\\sqrt{C}}$, but a single longer measurement saves the work of doing extra calculations* \n",
    "| tolerance (calibration)   | often neither will help | when the measurement equipment has a systematic uncertainty (say it consistently yields measurements that are too high - or too low), then performing more or longer measurements will not help\n",
    "\n",
    "**In the case of \"other type A uncertainties\", the measurement will fluctuate higher and lower due to some uncontrolled environmental conditions.  Integrating a factor of $C$ times longer is then equivalent to adding up $C$ shorter measurements, where the total signal grows by a factor $C$ but the uncertainty of this total signal grows by a factor $\\sqrt{u_o^2+u_o^2+u_o^2+u_o^2+...} = \\sqrt{C u_o^2} = u_o\\sqrt{C}$, where $u_o$ is the uncertainty of a short measurement.  When dividing both by $C$ to get the desired quantity, we see the standard uncertainty $\\frac{u_o}{\\sqrt{C}}$ is precisely the same as we get from increasing the number of measurements to $C \\cdot N$.  And, while we don't cover the Poisson distribution in these notes, a similar argument applies for it when our measurement consists of counting stochastic occurrences over a certain amount of time.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "********\n",
    "# TEASER:  Curve fitting & Fourier transform\n",
    "\n",
    "There is more to mathematical modeling of your experimental data than quantifying uncertainties.  In a subsequent set of lecture notes, you will see that Python has a user-friendly and flexible curve-fitting library.  Curve-fitting helps demonstrate relationships between physical quantities.  In fact also parameter values obtained in the curve-fiting of a set of measured data will have type A uncertainties (which is different than the form above which applies only to the measurand of repeated experiments).\n",
    "\n",
    "http://scipy-lectures.org/intro/scipy/auto_examples/plot_curve_fit.html\n",
    "\n",
    "And, if you are just looking for the Fourier transform, the numpy library already has you covered, and a simple application is treated in a subsequent Jupyter Notebook document.\n",
    "\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.fft.fft.html#numpy.fft.fft\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****\n",
    "# Acknowledgements\n",
    "\n",
    "Freek Pols (https://orcid.org/0000-0002-4690-6460) helped with the conception and development of these materials.\n",
    "\n",
    "Significant material, structure, and inspiration for this document has been drawn from the University of Cape Town's \"Introduction to Measurement in the Physics Laboratory:  A probabilistic approach\" by Buffler, Allie, Lubben, and Campbell.\n",
    "\n",
    "Additionally, some useful publications which informed these notes include:\n",
    "\n",
    "https://www.bipm.org/en/publications/guides/gum.html\n",
    "\n",
    "https://aapt.scitation.org/doi/10.1119/1.3023655\n",
    "\n",
    "Questions or suggestions can be sent to Forrest Bradbury (https://orcid.org/0000-0001-8412-4091) :  forrestbradbury (\"AT\") gmail.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "428.667px",
    "left": "518px",
    "top": "110.883px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
